{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvXdh4tfT12A",
        "outputId": "79e7ffca-d8f6-45d9-8082-cc0d7bfaa27e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8YuIfZQYFzi"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "on0LT5JUUerz",
        "outputId": "52be27ab-1985-4036-fc50-2956ddb9db9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai==0.28.0 in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28.0) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28.0) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28.0) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.0) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.0) (1.19.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.22.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n",
            "ERROR: unknown command \"instrall\" - maybe you meant \"install\"\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28.0\n",
        "!pip install huggingface_hub\n",
        "!pip install groq\n",
        "!pip instrall requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frULk7TKUoV1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from groq import Groq\n",
        "from huggingface_hub import InferenceClient\n",
        "import pandas as pd\n",
        "import json\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w0-XWWpKhyF"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj8MW4HNOED3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google.colab\n",
        "from huggingface_hub import InferenceClient, login, HfFolder\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "hf_token = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "\n",
        "if hf_token and not HfFolder.get_token():\n",
        "    login(token=hf_token)\n",
        "\n",
        "\n",
        "hf_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\", token=hf_token) #tokenizer\n",
        "\n",
        "#truncate the prompt\n",
        "def truncate_prompt(prompt, max_input_tokens=3900):\n",
        "    input_ids = hf_tokenizer(prompt, return_tensors=\"pt\").input_ids[0]\n",
        "    if len(input_ids) > max_input_tokens:\n",
        "        prompt = hf_tokenizer.decode(input_ids[:max_input_tokens], skip_special_tokens=True)\n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBu-LzjTXoRg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "import google.colab\n",
        "class Agent:\n",
        "    def __init__(self, name, role, llm_type):\n",
        "        self.name = name\n",
        "        self.role = role\n",
        "        self.llm_type = llm_type.lower()\n",
        "        self.memory = [] #memory for each agent is stored\n",
        "\n",
        "        #initialization of hugging face client\n",
        "        if self.llm_type == \"hf\":\n",
        "            self.hf_client = InferenceClient(\"google/flan-t5-base\") #google flan or 'microsoft/Phi-3-mini-4k-instruct'\n",
        "\n",
        "        elif self.llm_type == 'groq':\n",
        "          self.groq_api_key = \"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\"\n",
        "          self.groq_url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "\n",
        "\n",
        "    def add_to_memory(self, prompt, response):\n",
        "      self.memory.append({\"prompt\": prompt, \"response\": response})\n",
        "\n",
        "    def get_context(self, window_size=2): #remembers 2 converstaions\n",
        "      if not self.memory:\n",
        "        return \"\"  # is empty if there is no memory yet\n",
        "      context = \"\"\n",
        "      for item in self.memory[-window_size:]:\n",
        "          context += f\"User: {item['prompt']}\\n{self.name}: {item['response']}\\n\"\n",
        "      return context\n",
        "\n",
        "\n",
        "\n",
        "    def generate_response(self, prompt):\n",
        "        context = self.get_context()\n",
        "        full_prompt = context + f\"User: {prompt}\\n{self.name}:\" #total prompt including memory\n",
        "\n",
        "        if self.llm_type == 'hf':\n",
        "          response = self.query_huggingface(full_prompt)\n",
        "\n",
        "        elif self.llm_type == 'groq':\n",
        "          response = self.query_groq(full_prompt)\n",
        "          time.sleep(3)\n",
        "\n",
        "\n",
        "        else:\n",
        "            response = f\"{self.name} ({self.role}): Unsupported LLM type.\"\n",
        "\n",
        "        self.add_to_memory(prompt, response)\n",
        "        return response\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def query_huggingface(self, prompt):\n",
        "      try:\n",
        "\n",
        "        prompt = truncate_prompt(prompt, max_input_tokens=2048) #truncate the prompt\n",
        "        return self.hf_client.text_generation(prompt, max_new_tokens=300)\n",
        "      except Exception as e:\n",
        "\n",
        "        #if failed,try with a smaller truncated prompt\n",
        "        try:\n",
        "            return \"[HuggingFace Error - retrying with shorter prompt] \" + \\\n",
        "                   self.hf_client.text_generation(prompt[:300], max_new_tokens=30)\n",
        "        except Exception as e2:\n",
        "            return f\"[HF Final Error] {str(e2)}\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def query_groq(self, prompt):\n",
        "      try:\n",
        "\n",
        "          if len(prompt) > 7000 :\n",
        "            prompt = prompt[-7000:] #take 7000\n",
        "          headers = {\n",
        "            'Authorization': f'Bearer {self.groq_api_key}',\n",
        "            'Content-Type': 'application/json'\n",
        "          }\n",
        "          messages = [\n",
        "            {\"role\": \"system\", \"content\": f\"You are {self.name}, a {self.role} in a courtroom. Respond accordingly.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "          ]\n",
        "          payload = {\n",
        "              \"model\": \"llama-3.3-70b-versatile\",\n",
        "              \"messages\": messages,\n",
        "              \"max_tokens\": 300,\n",
        "              \"temperature\": 0.3,\n",
        "              \"top_p\" : 0.8,\n",
        "              \"stream\" : False\n",
        "          }\n",
        "          response = requests.post(\"https://api.groq.com/openai/v1/chat/completions\", headers=headers, json=payload)\n",
        "          response_json = response.json()\n",
        "          if response.status_code == 200:\n",
        "            return response_json['choices'][0]['message']['content'].strip()\n",
        "\n",
        "          elif response.status_code == 429:  # rate limit\n",
        "\n",
        "            if \"Retry-After\" in response.headers:\n",
        "                wait_time = float(response.headers[\"Retry-After\"])\n",
        "            else:\n",
        "                wait_time = 6  # fallback\n",
        "            print(f\"[Rate Limit] Waiting {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "            return self.query_groq(prompt)\n",
        "\n",
        "          else:\n",
        "            return f\"[Groq Error] {response_json.get('error', 'Unknown error')}\"\n",
        "      except Exception as e:\n",
        "          return f\"[Groq Error] {str(e)}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVHt-dCPY-Ss"
      },
      "outputs": [],
      "source": [
        "class CourtroomAgents:\n",
        "    def __init__(self):\n",
        "        self.agents = {}\n",
        "\n",
        "    def add_agent(self, name, role, llm_type):\n",
        "        agent = Agent(name, role, llm_type)\n",
        "        self.agents[role.lower()] = agent\n",
        "\n",
        "    def get_agent(self, role):\n",
        "        return self.agents.get(role.lower(), None)\n",
        "\n",
        "    def simulate_opening_statements(self, case_summary, accusation):\n",
        "        print(\" Trial Opening Statements\")\n",
        "\n",
        "        judge = self.get_agent('judge')\n",
        "        prosecutor = self.get_agent('prosecutor')\n",
        "        defense = self.get_agent('defense lawyer')\n",
        "        witness = self.get_agent('witness')\n",
        "        defendent = self.get_agent('defendent')\n",
        "        plaintiff = self.get_agent('plaintiff')\n",
        "        # Judge opens court\n",
        "        judge_prompt = f\"You are a judge. Open the courtroom and summarize the case precisely without showing any bias in not a long way: {case_summary}\"\n",
        "        casetype =judge.generate_response(f\"you have read the summary of the case : {case_summary} now tell what type of case it is(answer should be of the form (civil case) or (criminal case) in one word)\")\n",
        "        print(f\"============== {casetype} ====================\")\n",
        "        print(f\" Judge {judge.name}:\")\n",
        "        print(judge.generate_response(judge_prompt))\n",
        "\n",
        "        # Prosecutor gives opening statement\n",
        "        prosecutor_prompt = f\"You are a prosecutor. Present the accusations against the defendant correctly and shortly.Be confident and aggressive and express in a short way : {accusation}\"\n",
        "        print(f\"\\n Prosecutor {prosecutor.name}:\")\n",
        "        print(prosecutor.generate_response(prosecutor_prompt))\n",
        "\n",
        "        #plaintiff gives his side\n",
        "        if \"civil\" in casetype.lower():\n",
        "          plaintiff_prompt = f\"You are the plaintiff. Express your distress to the judge in a short way: {case_summary}\"\n",
        "          print(f\"\\n Plaintiff {plaintiff.name}:\")\n",
        "          print(plaintiff.generate_response(plaintiff_prompt))\n",
        "\n",
        "\n",
        "          # Defense gives opening statement\n",
        "          defense_prompt = f\"You are a defense lawyer. Begin your defense for the accused.Be calculative and answer more accurately and in a short way. Accusation: {accusation}\"\n",
        "          print(f\"\\n Defense Lawyer {defense.name}:\")\n",
        "          print(defense.generate_response(defense_prompt))\n",
        "\n",
        "    def simulate_witness_testimony(self, witness_name, role, testimony_prompt):\n",
        "        witness = self.get_agent(\"witness\")\n",
        "        print(f\"\\n {witness_name} ({role}) is called to testify.\")\n",
        "        testimony = witness.generate_response(f\"based on the following text produce a testimony for the witness in a short way:\\n\\n'{testimony_prompt}'\") #asking llm to observe a testimony in the text\n",
        "\n",
        "        response = witness.generate_response(f\"you need to Testify as a {role} on the following: {testimony}\") #witness gives his testification\n",
        "        print(f\"\\n{witness_name} testifies: {response}\")\n",
        "\n",
        "    def simulate_cross_examination(self, questioning_party, questioned_party, testimony):\n",
        "\n",
        "      questioning_agent = self.get_agent(questioning_party)\n",
        "      questioned_agent = self.get_agent(questioned_party)\n",
        "\n",
        "      # Ask LLM to generate a cross-exam question based on the testimony\n",
        "      question_prompt = f\"Based on the following testimony, generate a critical cross-examination question:\\n\\n'{testimony}'\"\n",
        "      question = questioning_agent.generate_response(question_prompt)\n",
        "\n",
        "      print(f\"\\n Cross-Examination by {questioning_party} of {questioned_party}\")\n",
        "      print(f\"{questioning_party}: {question}\")\n",
        "\n",
        "      # questioned party responds to the question\n",
        "      response = questioned_agent.generate_response(f\"{questioning_party} asks: {question} generate a precise response\")\n",
        "      print(f\"{questioned_party}: {response}\")\n",
        "\n",
        "    def simulate_closing_statements(self):\n",
        "        jury = self.get_agent('jury')\n",
        "        print(\"\\n Closing Statements\")\n",
        "\n",
        "        judge = self.get_agent('judge')\n",
        "        prosecutor = self.get_agent('prosecutor')\n",
        "        defense = self.get_agent('defense lawyer')\n",
        "\n",
        "        # Prosecutor closing statement\n",
        "        prosecutor_prompt = \"Make your closing argument and request the judge to take appropriate measures.\"\n",
        "        print(f\"\\n Prosecutor {prosecutor.name}:\")\n",
        "        print(prosecutor.generate_response(prosecutor_prompt))\n",
        "\n",
        "        # Defense closing statement\n",
        "        defense_prompt = \"Make your closing argument and request the judge to take appropriate measures.\"\n",
        "        print(f\"\\n Defense Lawyer {defense.name}:\")\n",
        "        print(defense.generate_response(defense_prompt))\n",
        "\n",
        "        # Judge gives the verdict\n",
        "        judge_prompt = \"Give the verdict on the case considering each and every detail in small and effective way.\"\n",
        "        print(f\"\\n Judge {judge.name}:\")\n",
        "        print(judge.generate_response(judge_prompt))\n",
        "\n",
        "        #jury gives the verdict\n",
        "        jury_prompt = \"learn about the whole case and inspect all details and tell whether the justice is done or not in a small way\"\n",
        "        print(f\"\\n Jury gives the statement: \\n\")\n",
        "        print(jury.generate_response(jury_prompt))\n",
        "\n",
        "    def predict_verdict(self, case_text):\n",
        "\n",
        "        judge = self.get_agent('judge')\n",
        "        if not judge:\n",
        "            return \"DENIED\"  # fallback\n",
        "\n",
        "        prompt = (\n",
        "            \"You are the judge. Read the following case summary and give the final verdict \"\n",
        "            \"in one word: GRANTED or DENIED.\\n\\n\"\n",
        "            f\"Case:\\n{case_text}\\n\\nVerdict:\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            response = judge.generate_response(prompt)\n",
        "        except Exception as e:\n",
        "            print(f\"[LLM Error]: {e}\")\n",
        "            return \"DENIED\"\n",
        "\n",
        "        if response is None:\n",
        "            return \"DENIED\"\n",
        "\n",
        "        return str(response).strip().upper()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVVZN8PAU0tX",
        "outputId": "6e7bd3a7-856e-4de8-bdda-d93dd8e07d41"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id,label\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing cases:   0%|          | 0/50 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Rate Limit] Waiting 311.0 seconds...\n",
            "Raw response for case 1989_75: AFTER CAREFUL CONSIDERATION OF THE ARGUMENTS PRESENTED AND THE PRINCIPLES OF NATURAL JUSTICE, I FIND THAT THE RESPONDENT'S ACTIONS WERE INDEED MISCONDUCT AND WARRANT SEVERE PUNISHMENT. THE RESPONDENT'S INVOLVEMENT IN THE FRAUDULENT ACT, PREPARATION OF BOGUS DOCUMENTS, AND SELF-AGGRANDIZEMENT DEMONSTRATE A CLEAR BREACH OF TRUST AND UNBECOMING CONDUCT FOR A GOVERNMENT SERVANT.\n",
            "\n",
            "WHILE THE TRIBUNAL IMPOSED A LESSER PENALTY, I BELIEVE THAT SUCH LENIENCY WOULD BE DETRIMENTAL TO THE INTEGRITY OF THE ADMINISTRATION. IT IS ESSENTIAL TO MAINTAIN THE HIGHEST STANDARDS OF CONDUCT AND INTEGRITY IN PUBLIC SERVICE, AND ANY COMPROMISE ON THIS FRONT WOULD UNDERMINE THE VERY FOUNDATIONS OF OUR DEMOCRACY.\n",
            "\n",
            "IN LIGHT OF THE PRINCIPLES DISCUSSED AND THE FACTS OF THE CASE, I HEREBY ALLOW THE APPEAL AND SET ASIDE THE ORDER OF THE TRIBUNAL. THE RESPONDENT'S ACTIONS JUSTIFY THE IMPOSITION OF A MORE SEVERE PENALTY, AND I HEREBY ORDER THAT THE ORIGINAL PENALTY BE REINSTATED.\n",
            "\n",
            "THE RESPONDENT'S CONDUCT HAS FALLEN SHORT OF THE EXPECTED STANDARDS, AND IT IS IMPERATIVE THAT WE UPHOLD THE INTEGRITY OF THE ADMINISTRATION. ANY SYMPATHY OR CHARITABLE VIEW TOWARDS SUCH OFFICIALS WOULD BE COUNTERPRODUCTIVE AND DETRIMENTAL TO THE SUCCESS OF OUR DEMOCRATIC SYSTEM.\n",
            "\n",
            "THEREFORE, THE APPEAL IS ALLOWED, AND THE ORDER OF THE TRIBUNAL IS SET ASIDE. THE ORIGINAL PENALTY SHALL BE REINSTATED, AND THE RESPONDENT SHALL FACE THE CONSEQUENCES OF HIS ACTIONS.\n",
            "\n",
            "THIS JUDGMENT IS FINAL AND BINDING. COURT IS ADJOURNED.\n",
            "1989_75,0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing cases:   2%|▏         | 1/50 [05:20<4:21:51, 320.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Rate Limit] Waiting 1741.0 seconds...\n",
            "Raw response for case 1959_102: AFTER CAREFUL CONSIDERATION OF THE FACTS AND THE PRINCIPLES OF NATURAL JUSTICE, I FIND THAT THE ENQUIRY HELD BY THE COMPANY ON MARCH 12 WAS INDEED FLAWED. THE COMPANY FAILED TO PROVIDE THE EMPLOYEE, DAS, WITH A FAIR OPPORTUNITY TO DEFEND HIMSELF AGAINST THE ALLEGATIONS MADE AGAINST HIM.\n",
            "\n",
            "THE PRINCIPLES OF NATURAL JUSTICE, AS LAID DOWN IN THE CASE OF UNION OF INDIA V. T.R. VARMA, REQUIRE THAT A PARTY SHOULD HAVE THE OPPORTUNITY TO ADDUCE ALL RELEVANT EVIDENCE, THAT THE EVIDENCE OF THE OPPONENT SHOULD BE TAKEN IN HIS PRESENCE, AND THAT HE SHOULD BE GIVEN THE OPPORTUNITY TO CROSS-EXAMINE THE WITNESSES EXAMINED BY THAT PARTY.\n",
            "\n",
            "IN THIS CASE, THE COMPANY FAILED TO EXAMINE THE WITNESSES IN THE PRESENCE OF DAS, AND DID NOT PROVIDE HIM WITH COPIES OF THEIR STATEMENTS OR READ THEM OUT TO HIM BEFORE ASKING HIM TO QUESTION THEM. THIS IS A CLEAR VIOLATION OF THE PRINCIPLES OF NATURAL JUSTICE.\n",
            "\n",
            "FURTHERMORE, THE COMPANY HAD THE OPPORTUNITY TO CURE THIS DEFECT BY PRODUCING THE WITNESSES BEFORE THE TRIBUNAL AND ALLOWING DAS TO CROSS-EXAMINE THEM, BUT FAILED TO DO SO. INSTEAD, THEY ONLY PRODUCED THE STATEMENTS RECORDED BY THE MANAGER, WHICH DID NOT PROVIDE DAS WITH A MEANINGFUL OPPORTUNITY TO RESPOND TO THE ALLEGATIONS AGAINST HIM.\n",
            "\n",
            "IN LIGHT OF THESE FINDINGS, I UPHOLD THE DECISION OF THE TRIBUNAL THAT THE ENQUIRY WAS NOT PROPER AND THAT THE COMPANY'S DECISION TO DISMISS DAS WAS UNJUSTIFIED. THE COMPANY'S FAILURE TO FOLLOW THE PRINCIPLES OF NATURAL JUSTICE HAS RESULTED IN A DENIAL OF JUSTICE TO THE EMPLOYEE, AND I HEREBY DECLARE THAT THE DISMISSAL\n",
            "1959_102,0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing cases:   4%|▍         | 2/50 [34:31<15:29:31, 1161.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Rate Limit] Waiting 1654.0 seconds...\n",
            "Raw response for case 1979_76: AFTER CAREFUL CONSIDERATION OF THE ARGUMENTS PRESENTED AND THE RELEVANT LAWS AND REGULATIONS, I, JUSTICE RAY, HEREBY DELIVER THE FOLLOWING VERDICT:\n",
            "\n",
            "THE APPELLANT, A QUASI-PERMANENT ALLOTTEE, IS ENTITLED TO COMPENSATION IN CASH FOR THE LAND TAKEN OVER BY THE CENTRAL GOVERNMENT, AS PER THE DECISION RECORDED IN THE PRESS NOTE DATED AUGUST 27, 1957. THIS DECISION, WHICH WAS JOINTLY TAKEN BY THE CENTRAL GOVERNMENT AND THE PUNJAB STATE GOVERNMENT, ALLOWS FOR PAYMENT OF COMPENSATION IN CASH TO QUASI-PERMANENT ALLOTTEES WHOSE LAND, MEASURING LESS THAN TWO STANDARD ACRES, IS ACQUIRED FOR PUBLIC PURPOSES.\n",
            "\n",
            "IN THIS CASE, THE APPELLANT'S LAND, MEASURING 1 STANDARD ACRE AND 15 1/2 UNITS, WAS TAKEN OVER BY THE CENTRAL GOVERNMENT IN JULY 1953. AS THE LAND ACQUIRED IS LESS THAN TWO STANDARD ACRES, THE APPELLANT IS ENTITLED TO COMPENSATION IN CASH, AS PER THE AFOREMENTIONED DECISION.\n",
            "\n",
            "THE RESPONDENTS' CONTENTION THAT COMPENSATION CAN ONLY BE PAID IN THE FORM OF LAND, AS PER RULE 49, IS NOT APPLICABLE IN THIS CASE, AS THE ALLOTMENT WAS MADE UNDER THE NOTIFICATION OF THE GOVERNMENT OF PUNJAB DATED JULY 8, 1949, AND CHAPTER VIII OF THE 1955 RULES, INCLUDING RULE 49, DOES NOT APPLY TO SUCH ALLOTMENTS.\n",
            "\n",
            "THEREFORE, I DIRECT THE RESPONDENTS TO PAY COMPENSATION IN CASH TO THE APPELLANT, AS PER THE DECISION RECORDED IN THE PRESS NOTE DATED AUGUST 27, 1957. THE QUANTUM OF COMPENSATION WILL BE WORKED OUT ACCORDING TO LAW AND THE MODALITIES\n",
            "1979_76,0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing cases:   6%|▌         | 3/50 [1:02:15<18:09:38, 1391.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Rate Limit] Waiting 1638.0 seconds...\n",
            "[Rate Limit] Waiting 1236.0 seconds...\n",
            "Raw response for case 1991_124: AFTER CAREFUL CONSIDERATION OF THE ARGUMENTS PRESENTED BY BOTH PARTIES AND A THOROUGH EXAMINATION OF THE RELEVANT DOCUMENTS AND RULES, I FIND THAT THE APPELLANT'S CLAIM TO BE APPOINTED AGAINST THE VACANCY ARISING LATER IS WITHOUT MERIT.\n",
            "\n",
            "THE RULES AND REGULATIONS GOVERNING THE INDIAN POLICE SERVICE, INCLUDING THE INDIAN POLICE SERVICE CADRE RULES 1954, THE INDIAN POLICE SERVICE RECRUITMENT RULES 1954, AND THE INDIAN POLICE SERVICE APPOINTMENT BY COMPETITIVE EXAMINATION REGULATIONS 1965, DO NOT SUPPORT THE APPELLANT'S CONTENTION THAT ALL NOTIFIED VACANCIES MUST BE FILLED.\n",
            "\n",
            "FURTHERMORE, THE AUTHORITIES' DECISION TO CLOSE THE PROCESS OF FINAL SELECTION AT A CERTAIN STAGE WAS NOT ARBITRARY, AND THE SUBSEQUENT APPOINTMENTS MADE TO OTHER SERVICES DO NOT ESTABLISH ANY ILLEGAL DISCRIMINATION AGAINST THE APPELLANT.\n",
            "\n",
            "IN LIGHT OF THE EVIDENCE PRESENTED, I REJECT THE APPELLANT'S ARGUMENT THAT THE RESPONDENT ACTED ARBITRARILY IN FILLING UP THE VACANCIES IN QUESTION. THE PROCESS OF FINAL SELECTION HAD TO BE CLOSED AT SOME STAGE, AND THE DECISION TO DO SO WAS TAKEN BEFORE THE APPELLANT'S CLAIM AROSE.\n",
            "\n",
            "THEREFORE, I DISMISS THE APPEAL AND UPHOLD THE DECISION OF THE RESPONDENT. THE APPELLANT HAS NOT ACQUIRED ANY RIGHT TO BE APPOINTED AGAINST THE VACANCY ARISING LATER, AND THE RESPONDENT'S ACTIONS WERE NOT ARBITRARY OR DISCRIMINATORY.\n",
            "\n",
            "THE APPEAL IS HEREBY DISMISSED.\n",
            "1991_124,0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing cases:   8%|▊         | 4/50 [1:50:18<25:18:12, 1980.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Rate Limit] Waiting 1541.0 seconds...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import sys\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/drive/My Drive/cases.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Select the first 50 cases\n",
        "data = data.head(50)\n",
        "\n",
        "# Print header for the output\n",
        "print(\"id,label\")\n",
        "\n",
        "# Loop through each case\n",
        "for i, row in tqdm(data.iterrows(), total=len(data), desc=\"Processing cases\"):\n",
        "    case_id = row['id'] if 'id' in row else i\n",
        "    case_summary = row['text']\n",
        "\n",
        "    # Create a new courtroom and add the judge\n",
        "    court = CourtroomAgents()\n",
        "    court.add_agent(\"Justice Ray\", \"Judge\", \"groq\")\n",
        "\n",
        "    # Call the 'predict_verdict' method to get the verdict for this case\n",
        "    try:\n",
        "        verdict = court.predict_verdict(case_summary)\n",
        "        print(f\"Raw response for case {case_id}: {verdict}\")  # Debug\n",
        "    except Exception as e:\n",
        "        print(f\"[Error for case {case_id}]: {str(e)}\", file=sys.stderr)\n",
        "        verdict = None  # Fallback for error\n",
        "\n",
        "    # Make sure verdict is a string\n",
        "    if isinstance(verdict, str):\n",
        "        verdict = verdict.strip().upper()\n",
        "    elif verdict is None:\n",
        "        verdict = \"DENIED\"  # Default fallback\n",
        "    else:\n",
        "        verdict = str(verdict).strip().upper()\n",
        "\n",
        "    # Convert final decision to label\n",
        "    if \"GRANTED\" in verdict:\n",
        "        label = 1\n",
        "    elif \"DENIED\" in verdict:\n",
        "        label = 0\n",
        "    else:\n",
        "        label = 0  # fallback\n",
        "\n",
        "    # Print result in required format\n",
        "    print(f\"{case_id},{label}\")\n",
        "\n",
        "    # Sleep to avoid rate limits\n",
        "    time.sleep(5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}